@techreport{RRBM.2020, 
year = {2020}, 
author = {(2017, Co-founders of RRBM. and 2020), revised}, 
title = {{A Vision of Responsible Research in Business and Management: Striving for Useful and Credible Knowledge}}, 
url = {https://rrbm.network/wp-content/uploads/2020/04/Position-Paper\_revised\_8April2020.pdf}, 
abstract = {{This position paper presents a vision of a future in which business schools and scholars worldwide have successfully transformed their research toward responsible science,2 producing
useful and credible knowledge that addresses problems important to business and society.3 This
vision is based on the belief that business can be a means for a better world if it is informed by
responsible research. The paper begins with a set of principles to support responsible research
and proposes actions by different stakeholders to help realize this vision. It explains the impetus
for the proposal by describing the current business research ecosystem, which encourages
research oriented toward scholarly impact much more than societal relevance. Changing the
incentives and culture around publications are essential to promoting responsible research.
Research is the foundation of business education and practice, yet business research has failed to
live up to its promise in promoting better policies and best practices. If nothing is done, business
research will lose its legitimacy at best; at worst, it will waste money, talent, and opportunity.
This paper ends with a call to action for directing research toward achieving humanityâ€™s highest
aspirations. The paper invites discussion and debate on the prospect of creating a responsible
research ecosystem to realize this future vision when business and management research has
become a force for change toward a better world.}}, 
keywords = {}
}
@MISC{banker2017,
  title        = "The Benefits of Leveraging Containers in the Financial
                  Services Industry",
  author       = "{International Banker}",
  abstract     = "When you hear the term ``containers'' in today's day and age,
                  your mind probably does not immediately think of large
                  standardized shipping vessels, designed and built for freight
                  transport across different modes of transport --- from ship
                  to rail to truck.",
  month        =  nov,
  year         =  2017,
  howpublished = "\url{https://internationalbanker.com/technology/benefits-leveraging-containers-financial-services-industry/}",
  note         = "Accessed: 2021-8-17",
  language     = "en"
}


@ARTICLE{Barredo_Arrieta2020,
  title    = "Explainable Artificial Intelligence ({XAI)}: Concepts,
              taxonomies, opportunities and challenges toward responsible {AI}",
  author   = "Barredo Arrieta, Alejandro and Diaz-Rodriguez, Natalia
              and Del Ser, Javier and Bennetot, Adrien and Tabik, Siham and
              Barbado, Alberto and Garcia, Salvador and Gil-Lopez, Sergio and
              Molina, Daniel and Benjamins, Richard and Chatila, Raja and
              Herrera, Francisco",
  abstract = "In the last few years, Artificial Intelligence (AI) has achieved
              a notable momentum that, if harnessed appropriately, may deliver
              the best of expectations over many application sectors across the
              field. For this to occur shortly in Machine Learning, the entire
              community stands in front of the barrier of explainability, an
              inherent problem of the latest techniques brought by
              sub-symbolism (e.g. ensembles or Deep Neural Networks) that were
              not present in the last hype of AI (namely, expert systems and
              rule based models). Paradigms underlying this problem fall within
              the so-called eXplainable AI (XAI) field, which is widely
              acknowledged as a crucial feature for the practical deployment of
              AI models. The overview presented in this article examines the
              existing literature and contributions already done in the field
              of XAI, including a prospect toward what is yet to be reached.
              For this purpose we summarize previous efforts made to define
              explainability in Machine Learning, establishing a novel
              definition of explainable Machine Learning that covers such prior
              conceptual propositions with a major focus on the audience for
              which the explainability is sought. Departing from this
              definition, we propose and discuss about a taxonomy of recent
              contributions related to the explainability of different Machine
              Learning models, including those aimed at explaining Deep
              Learning methods for which a second dedicated taxonomy is built
              and examined in detail. This critical literature analysis serves
              as the motivating background for a series of challenges faced by
              XAI, such as the interesting crossroads of data fusion and
              explainability. Our prospects lead toward the concept of
              Responsible Artificial Intelligence, namely, a methodology for
              the large-scale implementation of AI methods in real
              organizations with fairness, model explainability and
              accountability at its core. Our ultimate goal is to provide
              newcomers to the field of XAI with a thorough taxonomy that can
              serve as reference material in order to stimulate future research
              advances, but also to encourage experts and professionals from
              other disciplines to embrace the benefits of AI in their activity
              sectors, without any prior bias for its lack of interpretability.",
  journal  = "Inf. Fusion",
  volume   =  58,
  pages    = "82--115",
  month    =  jun,
  year     =  2020,
  keywords = "Explainable Artificial Intelligence; Machine Learning; Deep
              Learning; Data Fusion; Interpretability; Comprehensibility;
              Transparency; Privacy; Fairness; Accountability; Responsible
              Artificial Intelligence"
}


@techreport{SOA2020,
  author="{Society of Actuaries}",
  title        = "The Powerful Combination of Actuarial Expertise and
                  {InsurTech} Knowledge",
  abstract     = "The Society of Actuaries and Silicon Valley start-up
                  accelerator Plug and Play are creating a knowledge exchange
                  with our members and InsurTech start-ups to share expertise
                  and insights. These activities will start with a bi-monthly
                  webcast exclusive to SOA members, where they can interact
                  with InsurTech start-up executives.",
  institution ="Society of Actuaries",
  year         =  2020,
  howpublished = "\url{https://www.soa.org/programs/insurtech/}",
  note         = "Accessed: 2021-8-10"
}

@techreport{IBC2020,
  author        = "{Investment Banking Council}",
  title="{AI} in Investment banking - The New Frontier",
  abstract     = "AI is grabbing a central position in the world of investment
                  banking --- from equity trading to advisory. Learn about the
                  forays of AI in investment banking with real-life examples.",
  year         =  2020,
  howpublished = "\url{https://www.investmentbankingcouncil.org/blog/ai-in-investment-banking-the-new-frontier}",
  note         = "Accessed: 2021-8-10"
}



@ARTICLE{Jaeger2021,
  title     = "Interpretable machine learning for diversified portfolio
               construction",
  author    = "Jaeger, Markus and Kr{\"u}gel, Stephan and Marinelli, Dimitri
               and Papenbrock, Jochen and Schwendner, Peter",
  journal   = "The Journal of Financial Data Science",
  publisher = "Pageant Media US",
  pages     = "jfds.2021.1.066",
  month     =  jun,
  year      =  2021,
  language  = "en"
}

@ARTICLE{Abadie2017,
  title         = "The Risk of Machine Learning",
  author        = "Abadie, Alberto and Kasy, Maximilian",
  abstract      = "Many applied settings in empirical economics involve
                   simultaneous estimation of a large number of parameters. In
                   particular, applied economists are often interested in
                   estimating the effects of many-valued treatments (like
                   teacher effects or location effects), treatment effects for
                   many groups, and prediction models with many regressors. In
                   these settings, machine learning methods that combine
                   regularized estimation and data-driven choices of
                   regularization parameters are useful to avoid over-fitting.
                   In this article, we analyze the performance of a class of
                   machine learning estimators that includes ridge, lasso and
                   pretest in contexts that require simultaneous estimation of
                   many parameters. Our analysis aims to provide guidance to
                   applied researchers on (i) the choice between regularized
                   estimators in practice and (ii) data-driven selection of
                   regularization parameters. To address (i), we characterize
                   the risk (mean squared error) of regularized estimators and
                   derive their relative performance as a function of simple
                   features of the data generating process. To address (ii), we
                   show that data-driven choices of regularization parameters,
                   based on Stein's unbiased risk estimate or on
                   cross-validation, yield estimators with risk uniformly close
                   to the risk attained under the optimal (unfeasible) choice
                   of regularization parameters. We use data from recent
                   examples in the empirical economics literature to illustrate
                   the practical applicability of our results.",
  month         =  mar,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1703.10935"
}


@ARTICLE{Wolpert1997,
  title    = "No free lunch theorems for optimization",
  author   = "Wolpert, D H and Macready, W G",
  abstract = "A framework is developed to explore the connection between
              effective optimization algorithms and the problems they are
              solving. A number of ``no free lunch'' (NFL) theorems are
              presented which establish that for any algorithm, any elevated
              performance over one class of problems is offset by performance
              over another class. These theorems result in a geometric
              interpretation of what it means for an algorithm to be well
              suited to an optimization problem. Applications of the NFL
              theorems to information-theoretic aspects of optimization and
              benchmark measures of performance are also presented. Other
              issues addressed include time-varying optimization problems and a
              priori ``head-to-head'' minimax distinctions between optimization
              algorithms, distinctions that result despite the NFL theorems'
              enforcing of a type of uniformity over all algorithms.",
  journal  = "IEEE Trans. Evol. Comput.",
  volume   =  1,
  number   =  1,
  pages    = "67--82",
  month    =  apr,
  year     =  1997,
  keywords = "Iron;Evolutionary computation;Information theory;Minimax
              techniques;Simulated annealing;Algorithm design and
              analysis;Performance analysis;Probability distribution;Bayesian
              methods"
}


@ARTICLE{Apley2020,
  title     = "Visualizing the effects of predictor variables in black box
               supervised learning models",
  author    = "Apley, Daniel W and Zhu, Jingyu",
  journal   = "J. R. Stat. Soc. Series B Stat. Methodol.",
  publisher = "Wiley",
  volume    =  82,
  number    =  4,
  pages     = "1059--1086",
  month     =  sep,
  year      =  2020,
  copyright = "http://onlinelibrary.wiley.com/termsAndConditions\#vor",
  language  = "en"
}

@ARTICLE{Athey2019a,
  title     = "Machine Learning Methods That Economists Should Know About",
  author    = "Athey, Susan and Imbens, Guido W",
  abstract  = "We discuss the relevance of the recent machine learning (ML)
               literature for economics and econometrics. First we discuss the
               differences in goals, methods, and settings between the ML
               literature and the traditional econometrics and statistics
               literatures. Then we discuss some specific methods from the ML
               literature that we view as important for empirical researchers
               in economics. These include supervised learning methods for
               regression and classification, unsupervised learning methods,
               and matrix completion methods. Finally, we highlight newly
               developed methods at the intersection of ML and econometrics
               that typically perform better than either off-the-shelf ML or
               more traditional econometric methods when applied to particular
               classes of problems, including causal inference for average
               treatment effects, optimal policy estimation, and estimation of
               the counterfactual effect of price changes in consumer choice
               models.",
  journal   = "Annu. Rev. Econom.",
  publisher = "Annual Reviews",
  volume    =  11,
  number    =  1,
  pages     = "685--725",
  month     =  aug,
  year      =  2019,
  language  = "en"
}

@ARTICLE{Athey2019b,
  title         = "Ensemble Methods for Causal Effects in Panel Data Settings",
  author        = "Athey, Susan and Bayati, Mohsen and Imbens, Guido and Qu,
                   Zhaonan",
  abstract      = "This paper studies a panel data setting where the goal is to
                   estimate causal effects of an intervention by predicting the
                   counterfactual values of outcomes for treated units, had
                   they not received the treatment. Several approaches have
                   been proposed for this problem, including regression
                   methods, synthetic control methods and matrix completion
                   methods. This paper considers an ensemble approach, and
                   shows that it performs better than any of the individual
                   methods in several economic datasets. Matrix completion
                   methods are often given the most weight by the ensemble, but
                   this clearly depends on the setting. We argue that ensemble
                   methods present a fruitful direction for further research in
                   the causal panel data setting.",
  month         =  mar,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "econ.EM",
  eprint        = "1903.10079"
}

@ARTICLE{Athey2017,
  title         = "Policy Learning with Observational Data",
  author        = "Athey, Susan and Wager, Stefan",
  abstract      = "In many areas, practitioners seek to use observational data
                   to learn a treatment assignment policy that satisfies
                   application-specific constraints, such as budget, fairness,
                   simplicity, or other functional form constraints. For
                   example, policies may be restricted to take the form of
                   decision trees based on a limited set of easily observable
                   individual characteristics. We propose a new approach to
                   this problem motivated by the theory of semiparametrically
                   efficient estimation. Our method can be used to optimize
                   either binary treatments or infinitesimal nudges to
                   continuous treatments, and can leverage observational data
                   where causal effects are identified using a variety of
                   strategies, including selection on observables and
                   instrumental variables. Given a doubly robust estimator of
                   the causal effect of assigning everyone to treatment, we
                   develop an algorithm for choosing whom to treat, and
                   establish strong guarantees for the asymptotic utilitarian
                   regret of the resulting policy.",
  month         =  feb,
  year          =  2017,
  archivePrefix = "arXiv",
  primaryClass  = "math.ST",
  eprint        = "1702.02896"
}

@ARTICLE{Athey2019c,
  title     = "Generalized random forests",
  author    = "Athey, Susan and Tibshirani, Julie and Wager, Stefan",
  abstract  = "We propose generalized random forests, a method for
               nonparametric statistical estimation based on random forests
               (Breiman [Mach. Learn. 45 (2001) 5--32]) that can be used to fit
               any quantity of interest identified as the solution to a set of
               local moment equations. Following the literature on local
               maximum likelihood estimation, our method considers a weighted
               set of nearby training examples; however, instead of using
               classical kernel weighting functions that are prone to a strong
               curse of dimensionality, we use an adaptive weighting function
               derived from a forest designed to express heterogeneity in the
               specified quantity of interest. We propose a flexible,
               computationally efficient algorithm for growing generalized
               random forests, develop a large sample theory for our method
               showing that our estimates are consistent and asymptotically
               Gaussian and provide an estimator for their asymptotic variance
               that enables valid confidence intervals. We use our approach to
               develop new methods for three statistical tasks: nonparametric
               quantile regression, conditional average partial effect
               estimation and heterogeneous treatment effect estimation via
               instrumental variables. A software implementation, grf for R and
               C++, is available from CRAN.",
  journal   = "aos",
  publisher = "Institute of Mathematical Statistics",
  volume    =  47,
  number    =  2,
  pages     = "1148--1178",
  month     =  apr,
  year      =  2019,
  keywords  = "62G05; Asymptotic theory; Causal inference; instrumental
               variable;",
  language  = "en"
}

@ARTICLE{Bachelier1900,
  title   = "Theory of speculation in the random character of stock market
             prices",
  author  = "Bachelier, Louis",
  journal = "MIT Press, Cambridge, Mass. Blattberg",
  volume  =  1018,
  pages   = "17--78",
  year    =  1900
}


@ARTICLE{Baumer2014,
  title    = "{R} Markdown: Integrating A Reproducible Analysis Tool into
              Introductory Statistics",
  author   = "Baumer, B and {\c C}etinkaya-Rundel, Mine and Bray, Andrew and
              Loi, Linda and Horton, N",
  abstract = "Nolan and Temple Lang argue that ``the ability to express
              statistical computations is an essential skill.'' A key related
              capacity is the ability to conduct and present data analysis in a
              way that another person can understand and replicate. The
              copy-and-paste workflow that is an artifact of antiquated
              user-interface design makes reproducibility of statistical
              analysis more difficult, especially as data become increasingly
              complex and statistical methods become increasingly
              sophisticated. R Markdown is a new technology that makes creating
              fully-reproducible statistical analysis simple and painless. It
              provides a solution suitable not only for cutting edge research,
              but also for use in an introductory statistics course. We present
              evidence that R Markdown can be used effectively in introductory
              statistics courses, and discuss its role in the rapidly-changing
              world of statistical computation.",
  journal  = "undefined",
  year     =  2014
}

@ARTICLE{Black1973,
  title     = "The Pricing of Options and Corporate Liabilities",
  author    = "Black, Fischer and Scholes, Myron",
  abstract  = "If options are correctly priced in the market, it should not be
               possible to make sure profits by creating portfolios of long and
               short positions in options and their underlying stocks. Using
               this principle, a theoretical valuation formula for options is
               derived. Since almost all corporate liabilities can be viewed as
               combinations of options, the formula and the analysis that led
               to it are also applicable to corporate liabilities such as
               common stock, corporate bonds, and warrants. In particular, the
               formula can be used to derive the discount that should be
               applied to a corporate bond because of the possibility of
               default.",
  journal   = "J. Polit. Econ.",
  publisher = "The University of Chicago Press",
  volume    =  81,
  number    =  3,
  pages     = "637--654",
  month     =  may,
  year      =  1973
}

@ARTICLE{Breiman2001,
  title    = "Statistical Modeling: The Two Cultures (with comments and a
              rejoinder by the author)",
  author   = "Breiman, Leo",
  abstract = "There are two cultures in the use of statistical modeling to
              reach conclusions from data. One assumes that the data are
              generated by a given stochastic data model. The other uses
              algorithmic models and treats the data mechanism as unknown. The
              statistical community has been committed to the almost exclusive
              use of data models. This commitment has led to irrelevant theory,
              questionable conclusions, and has kept statisticians from working
              on a large range of interesting current problems. Algorithmic
              modeling, both in theory and practice, has developed rapidly in
              fields outside statistics. It can be used both on large complex
              data sets and as a more accurate and informative alternative to
              data modeling on smaller data sets. If our goal as a field is to
              use data to solve problems, then we need to move away from
              exclusive dependence on data models and adopt a more diverse set
              of tools.",
  journal  = "Statistical Science",
  volume   =  16,
  number   =  3,
  pages    = "199--231",
  month    =  aug,
  year     =  2001,
  language = "en"
}

@ARTICLE{Bzdok2018,
  title     = "Statistics versus machine learning",
  author    = "Bzdok, Danilo and Altman, Naomi and Krzywinski, Martin",
  journal   = "Nat. Methods",
  publisher = "Springer Science and Business Media LLC",
  volume    =  15,
  number    =  4,
  pages     = "233--234",
  month     =  apr,
  year      =  2018,
  language  = "en"
}

@ARTICLE{Cesa2017,
  title     = "A brief history of quantitative finance",
  author    = "Cesa, Mauro",
  abstract  = "In this introductory paper to the issue, I will travel through
               the history of how quantitative finance has developed and
               reached its current status, what problems it is called to
               address, and how they differ from those of the pre-crisis world.",
  journal   = "Probability, Uncertainty and Quantitative Risk",
  publisher = "SpringerOpen",
  volume    =  2,
  number    =  1,
  pages     = "1--16",
  month     =  jun,
  year      =  2017,
  language  = "en"
}

@ARTICLE{Chamberlain2020,
  title     = "Robust Decision Theory and Econometrics",
  author    = "Chamberlain, Gary",
  abstract  = "This review uses the empirical analysis of portfolio choice to
               illustrate econometric issues that arise in decision problems.
               Subjective expected utility (SEU) can provide normative guidance
               to an investor making a portfolio choice. The investor, however,
               may have doubts on the specification of the distribution and may
               seek a decision theory that is less sensitive to the
               specification. I consider three such theories: maxmin expected
               utility, variational preferences (including multiplier and
               divergence preferences and the associated constraint
               preferences), and smooth ambiguity preferences. I use a simple
               two-period model to illustrate their application. Normative
               empirical work on portfolio choice is mainly in the SEU
               framework, and bringing in ideas from robust decision theory may
               be fruitful.",
  journal   = "Annu. Rev. Econom.",
  publisher = "Annual Reviews",
  volume    =  12,
  number    =  1,
  pages     = "239--271",
  month     =  aug,
  year      =  2020
}

@ARTICLE{Chamberlain2000,
  title    = "Econometrics and decision theory",
  author   = "Chamberlain, Gary",
  abstract = "The paper considers the role of econometrics in decision making
              under uncertainty. This leads to a focus on predictive
              distributions. The decision maker's subjective distribution is
              only partly specified; it belongs to a set S of distributions. S
              can also be regarded as a set of plausible data-generating
              processes. Criteria are needed to evaluate procedures for
              constructing predictive distributions. We use risk robustness and
              minimax regret risk relative to S. To obtain procedures for
              constructing predictive distributions, we use Bayes procedures
              based on parametric models with approximate prior distributions.
              The priors are nested, with a first stage that incorporates
              qualitative information such as exchangeability, and a second
              stage that is quite diffuse. Special points in the parameter
              space, such as boundary points, can be accommodated with
              second-stage priors that have one or more mass points but are
              otherwise quite diffuse. An application of these ideas is
              presented, motivated by an individual's consumption decision. The
              problem is to construct a distribution for that individual's
              future earnings, based on his earnings history and on a
              longitudinal data set that provides earnings histories for a
              sample of individuals.",
  journal  = "J. Econom.",
  volume   =  95,
  number   =  2,
  pages    = "255--283",
  month    =  apr,
  year     =  2000,
  keywords = "Expected utility; Predictive distribution; Risk robustness;
              Minimax regret; Bayes procedure; Longitudinal data"
}

@MISC{Dreze1972,
  title   = "Econometrics and Decision Theory",
  author  = "Dreze, Jacques H",
  journal = "Econometrica",
  volume  =  40,
  number  =  1,
  pages   = "1",
  year    =  1972
}

@BOOK{Efron2016,
  title     = "Computer Age Statistical Inference",
  author    = "Efron, Bradley and Hastie, Trevor",
  abstract  = "The twenty-first century has seen a breathtaking expansion of
               statistical methodology, both in scope and in influence. 'Big
               data', 'data science', and 'machine learning' have become
               familiar terms in the news, as statistical methods are brought
               to bear upon the enormous data sets of modern science and
               commerce. How did we get here? And where are we going? This book
               takes us on an exhilarating journey through the revolution in
               data analysis following the introduction of electronic
               computation in the 1950s. Beginning with classical inferential
               theories - Bayesian, frequentist, Fisherian - individual
               chapters take up a series of influential topics: survival
               analysis, logistic regression, empirical Bayes, the jackknife
               and bootstrap, random forests, neural networks, Markov chain
               Monte Carlo, inference after model selection, and dozens more.
               The distinctly modern approach integrates methodology and
               algorithms with statistical inference. The book ends with
               speculation on the future direction of statistics and data
               science.",
  publisher = "Cambridge University Press",
  month     =  jul,
  year      =  2016,
  language  = "en"
}

@BOOK{Hastie2009,
  title     = "The Elements of Statistical Learning: Data Mining, Inference,
               and Prediction, Second Edition",
  author    = "Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome",
  abstract  = "During the past decade there has been an explosion in
               computation and information technology. With it have come vast
               amounts of data in a variety of fields such as medicine,
               biology, finance, and marketing. The challenge of understanding
               these data has led to the development of new tools in the field
               of statistics, and spawned new areas such as data mining,
               machine learning, and bioinformatics. Many of these tools have
               common underpinnings but are often expressed with different
               terminology. This book describes the important ideas in these
               areas in a common conceptual framework. While the approach is
               statistical, the emphasis is on concepts rather than
               mathematics. Many examples are given, with a liberal use of
               color graphics. It is a valuable resource for statisticians and
               anyone interested in data mining in science or industry. The
               book's coverage is broad, from supervised learning (prediction)
               to unsupervised learning. The many topics include neural
               networks, support vector machines, classification trees and
               boosting---the first comprehensive treatment of this topic in
               any book. This major new edition features many topics not
               covered in the original, including graphical models, random
               forests, ensemble methods, least angle regression \& path
               algorithms for the lasso, non-negative matrix factorization, and
               spectral clustering. There is also a chapter on methods for
               ``wide'' data (p bigger than n), including multiple testing and
               false discovery rates. Trevor Hastie, Robert Tibshirani, and
               Jerome Friedman are professors of statistics at Stanford
               University. They are prominent researchers in this area: Hastie
               and Tibshirani developed generalized additive models and wrote a
               popular book of that title. Hastie co-developed much of the
               statistical modeling software and environment in R/S-PLUS and
               invented principal curves and surfaces. Tibshirani proposed the
               lasso and is co-author of the very successful An Introduction to
               the Bootstrap. Friedman is the co-inventor of many data-mining
               tools including CART, MARS, projection pursuit and gradient
               boosting.",
  publisher = "Springer Science \& Business Media",
  month     =  aug,
  year      =  2009,
  language  = "en"
}

@ARTICLE{Israel2020,
  title     = "Can machines 'learn' finance?",
  author    = "Israel, Ronen and Kelly, Bryan T and Moskowitz, Tobias J",
  abstract  = "Machine learning for asset management faces a unique set of
               challenges that differ markedly from other domains where machine
               learning has excelled. Understanding these differences is
               critical for developing impactful approaches and realistic
               expectations for machine learning in asset management. We
               discuss a variety of beneficial use cases and potential
               pitfalls, and emphasize the importance of economic theory and
               human expertise for achieving success through financial machine
               learning.",
  journal   = "SSRN Electron. J.",
  publisher = "Elsevier BV",
  month     =  jan,
  year      =  2020,
  language  = "en"
}

@BOOK{James2013,
  title     = "An Introduction to Statistical Learning: with Applications in
               {R}",
  author    = "James, Gareth and Witten, Daniela and Hastie, Trevor and
               Tibshirani, Robert",
  abstract  = "An Introduction to Statistical Learning provides an accessible
               overview of the field of statistical learning, an essential
               toolset for making sense of the vast and complex data sets that
               have emerged in fields ranging from biology to finance to
               marketing to astrophysics in the past twenty years. This book
               presents some of the most important modeling and prediction
               techniques, along with relevant applications. Topics include
               linear regression, classification, resampling methods, shrinkage
               approaches, tree-based methods, support vector machines,
               clustering, and more. Color graphics and real-world examples are
               used to illustrate the methods presented. Since the goal of this
               textbook is to facilitate the use of these statistical learning
               techniques by practitioners in science, industry, and other
               fields, each chapter contains a tutorial on implementing the
               analyses and methods presented in R, an extremely popular open
               source statistical software platform.Two of the authors co-wrote
               The Elements of Statistical Learning (Hastie, Tibshirani and
               Friedman, 2nd edition 2009), a popular reference book for
               statistics and machine learning researchers. An Introduction to
               Statistical Learning covers many of the same topics, but at a
               level accessible to a much broader audience. This book is
               targeted at statisticians and non-statisticians alike who wish
               to use cutting-edge statistical learning techniques to analyze
               their data. The text assumes only a previous course in linear
               regression and no knowledge of matrix algebra.",
  publisher = "Springer Science \& Business Media",
  month     =  jun,
  year      =  2013,
  language  = "en"
}

@ARTICLE{Kaplan2007,
  title    = "Computing and Introductory Statistics",
  author   = "Kaplan, Daniel",
  abstract = "Author(s): Kaplan, Daniel | Abstract: Much of the computing that
              students do in introductory statistics courses is based on
              techniques that were developed before computing became
              inexpensive and ubiquitous. Now that computing is readily
              available to all students, instructors can change the way we
              teach statistical concepts. This article describes computational
              ideas that can support teaching George Cobb's Three Rs of
              statistical inference: Randomize, Repeat, Reject.",
  journal  = "Technology Innovations in Statistics Education",
  volume   =  1,
  number   =  1,
  month    =  oct,
  year     =  2007
}

@BOOK{Pearl2009,
  title     = "Causality",
  author    = "Pearl, Judea",
  abstract  = "Written by one of the preeminent researchers in the field, this
               book provides a comprehensive exposition of modern analysis of
               causation. It shows how causality has grown from a nebulous
               concept into a mathematical theory with significant applications
               in the fields of statistics, artificial intelligence, economics,
               philosophy, cognitive science, and the health and social
               sciences. Judea Pearl presents and unifies the probabilistic,
               manipulative, counterfactual, and structural approaches to
               causation and devises simple mathematical tools for studying the
               relationships between causal connections and statistical
               associations. The book will open the way for including causal
               analysis in the standard curricula of statistics, artificial
               intelligence, business, epidemiology, social sciences, and
               economics. Students in these fields will find natural models,
               simple inferential procedures, and precise mathematical
               definitions of causal concepts that traditional texts have
               evaded or made unduly complicated. The first edition of
               Causality has led to a paradigmatic change in the way that
               causality is treated in statistics, philosophy, computer
               science, social science, and economics. Cited in more than 5,000
               scientific publications, it continues to liberate scientists
               from the traditional molds of statistical thinking. In this
               revised edition, Judea Pearl elucidates thorny issues, answers
               readers' questions, and offers a panoramic view of recent
               advances in this field of research. Causality will be of
               interests to students and professionals in a wide variety of
               fields. Anyone who wishes to elucidate meaningful relationships
               from data, predict effects of actions and policies, assess
               explanations of reported events, or form theories of causal
               understanding and causal speech will find this book stimulating
               and invaluable.",
  publisher = "Cambridge University Press",
  month     =  sep,
  year      =  2009,
  language  = "en"
}

@ARTICLE{Mavridis2019,
  title    = "Combining containers and virtual machines to enhance isolation
              and extend functionality on cloud computing",
  author   = "Mavridis, Ilias and Karatza, Helen",
  abstract = "Virtualization technology is the underlying element of cloud
              computing. Traditionally, cloud computing has employed virtual
              machines to distribute available resources and provide isolated
              environments among users. Multiple virtual machines, with their
              own operating system and services, can be deployed and run
              simultaneously on the same physical machine on the cloud
              infrastructure. Recently, a more lightweight virtualization
              technology is being rapidly adopted and it is based on
              containers. A key difference between virtual machines and
              containers, is that containers share the same underlying
              operating system. In this work, we studied the combination of
              these two virtualization technologies by running containers on
              top of virtual machines. This synergy aims to enhance containers'
              main drawback, which is isolation and, among others, to simplify
              the system management and upgrade, and to introduce the new
              functionalities of containerized applications to virtual
              machines. The benefits of this method have been recognized by big
              cloud companies, which have been employing this approach for
              years. With this paper, we aimed to present the advantages of
              running containers on virtual machines and to explore how various
              virtualization techniques and configurations affect the
              performance of this method. Although, in bibliography there are a
              few papers that used this method partially to conduct
              experiments, there is not any research which considers this
              method from our integral aspect of view. In this study, we ran
              Docker containers and evaluated their performances, not only on
              KVM and XEN virtual machines, but also we ran Linux containers on
              Windows Server. We adopted an empirical approach, to quantify the
              performance overhead introduced by the additional virtualization
              layer of virtual machines, by executing various benchmarks and
              deploying real world applications as use cases. Furthermore, we
              presented for first time, how isolation is applied on virtual
              machines and containers, we evaluated different operating systems
              optimized to host containers, mechanisms of storing persistent
              data and, last but not least, we experimentally quantified the
              power and energy consumption overhead of containers running on
              virtual machines.",
  journal  = "Future Gener. Comput. Syst.",
  volume   =  94,
  pages    = "674--696",
  month    =  may,
  year     =  2019,
  keywords = "Docker; VMs; Containers; KVM; XEN; Hyper-V; Performance
              evaluation; Energy consumption"
}

@MISC{banker2021,
  title        = "Does a {DARQ} Future Await the World?",
  author       = "{internationalbanker}",
  abstract     = "New technologies have infiltrated every corner of the world
                  during the digital age, but four stand out as frontrunners.
                  The DARQ forces of distributed ledger technology, artificial
                  intelligence, extended reality and quantum computing, working
                  independently and together, are ready to guide businesses and
                  society into a bold new frontier.",
  month        =  jun,
  year         =  2021,
  howpublished = "\url{https://internationalbanker.com/technology/does-a-darq-future-await-the-world/}",
  note         = "Accessed: 2021-8-18",
  language     = "en"
}

@ARTICLE{Lommers2021,
  title     = "Confronting machine learning with financial research",
  author    = "Lommers, Kristof and Harzli, Ouns El and Kim, Jack",
  journal   = "The Journal of Financial Data Science",
  publisher = "Pageant Media US",
  pages     = "jfds.2021.1.068",
  month     =  jun,
  year      =  2021,
  language  = "en"
}

@ARTICLE{Dixon2020,
  title         = "Deep Fundamental Factor Models",
  author        = "Dixon, Matthew F and Polson, Nicholas G",
  abstract      = "Deep fundamental factor models are developed to
                   automatically capture non-linearity and interaction effects
                   in factor modeling. Uncertainty quantification provides
                   interpretability with interval estimation, ranking of factor
                   importances and estimation of interaction effects. With no
                   hidden layers we recover a linear factor model and for one
                   or more hidden layers, uncertainty bands for the sensitivity
                   to each input naturally arise from the network weights.
                   Using 3290 assets in the Russell 1000 index over a period of
                   December 1989 to January 2018, we assess a 49 factor model
                   and generate information ratios that are approximately 1.5x
                   greater than the OLS factor model. Furthermore, we compare
                   our deep fundamental factor model with a quadratic LASSO
                   model and demonstrate the superior performance and
                   robustness to outliers. The Python source code and the data
                   used for this study are provided.",
  month         =  mar,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1903.07677"
}


@ARTICLE{Easley2020,
  title     = "Microstructure in the Machine Age",
  author    = "Easley, David and L{\'o}pez de Prado, Marcos and O'Hara, Maureen
               and Zhang, Zhibai",
  abstract  = "Abstract. Understanding modern market microstructure phenomena
               requires large amounts of data and advanced mathematical tools.
               We demonstrate how machine learni",
  journal   = "Rev. Financ. Stud.",
  publisher = "Oxford University Press",
  month     =  jul,
  year      =  2020,
  language  = "en"
}



@BOOK{Lopez_de_Prado2018,
  title     = "Advances in Financial Machine Learning",
  author    = "L{\'o}pez de Prado, Marcos",
  abstract  = "Machine learning (ML) is changing virtually every aspect of our
               lives. Today ML algorithms accomplish tasks that until recently
               only expert humans could perform. As it relates to finance, this
               is the most exciting time to adopt a disruptive technology that
               will transform how everyone invests for generations. Readers
               will learn how to structure Big data in a way that is amenable
               to ML algorithms; how to conduct research with ML algorithms on
               that data; how to use supercomputing methods; how to backtest
               your discoveries while avoiding false positives. The book
               addresses real-life problems faced by practitioners on a daily
               basis, and explains scientifically sound solutions using math,
               supported by code and examples. Readers become active users who
               can test the proposed solutions in their particular setting.
               Written by a recognized expert and portfolio manager, this book
               will equip investment professionals with the groundbreaking
               tools needed to succeed in modern finance.",
  publisher = "John Wiley \& Sons",
  month     =  feb,
  year      =  2018,
  language  = "en"
}

@BOOK{Cunningham2021,
  title     = "Causal inference: The mixtape",
  author    = "Cunningham, Scott",
  abstract  = "Ashenfelter, O., 415--423 Assumptions: common support, 209, 221;
               conditional independence assumption (CIA), 176, 208--209,
               221n14, 224--226, 228, 240; continuity, 245, 255--260; defined,
               101; identifying, 182--183, 393; independence, 135--140;
               monotonicity, 350; parallel trend, 413, 422, 425--433; parallel
               trends, 413; stable unit treatment value assumption (SUTVA),
               140--141, 348, 348n13, 478n16",
  publisher = "Yale University Press",
  year      =  2021
}

@ARTICLE{Zuo2021,
  title     = "Variable selection with second-generation P-values",
  author    = "Zuo, Yi and Stewart, Thomas G and Blume, Jeffrey D",
  journal   = "Am. Stat.",
  publisher = "Informa UK Limited",
  pages     = "1--21",
  month     =  jun,
  year      =  2021,
  language  = "en"
}


@ARTICLE{Varghese2019,
  title         = "Cloud Futurology",
  author        = "Varghese, Blesson and Leitner, Philipp and Ray, Suprio and
                   Chard, Kyle and Barker, Adam and Elkhatib, Yehia and Herry,
                   Herry and Hong, Cheol-Ho and Singer, Jeremy and Tso, Fung Po
                   and Yoneki, Eiko and Zhani, Mohamed-Faten",
  abstract      = "The Cloud has become integral to most Internet-based
                   applications and user gadgets. This article provides a brief
                   history of the Cloud and presents a researcher's view of the
                   prospects for innovating at the infrastructure, middleware,
                   and application and delivery levels of the already crowded
                   Cloud computing stack.",
  month         =  feb,
  year          =  2019,
  archivePrefix = "arXiv",
  primaryClass  = "cs.DC",
  eprint        = "1902.03656"
}


@ARTICLE{Rubin1974,
  title    = "Estimating causal effects of treatments in randomized and
              nonrandomized studies",
  author   = "Rubin, Donald B",
  abstract = "Presents a discussion of matching, randomization, random
              sampling, and other methods of controlling extraneous variation.
              The objective was to specify the benefits of randomization in
              estimating causal effects of treatments. It is concluded that
              randomization should be employed whenever possible but that the
              use of carefully controlled nonrandomized data to estimate causal
              effects is a reasonable and necessary procedure in many cases.
              (15 ref) (PsycINFO Database Record (c) 2016 APA, all rights
              reserved)",
  journal  = "J. Educ. Psychol.",
  volume   =  66,
  number   =  5,
  pages    = "688--701",
  month    =  oct,
  year     =  1974
}


@ARTICLE{Wright1934,
  title     = "The Method of Path Coefficients",
  author    = "Wright, Sewall",
  abstract  = "The Annals of Mathematical Statistics",
  journal   = "aoms",
  publisher = "Institute of Mathematical Statistics",
  volume    =  5,
  number    =  3,
  pages     = "161--215",
  month     =  sep,
  year      =  1934,
  language  = "en"
}


@ARTICLE{Haavelmo1943,
  title     = "The Statistical Implications of a System of Simultaneous
               Equations",
  author    = "Haavelmo, Trygve",
  journal   = "Econometrica",
  publisher = "[Wiley, Econometric Society]",
  volume    =  11,
  number    =  1,
  pages     = "1--12",
  year      =  1943
}


@BOOK{Dixon2020a,
  title     = "Machine Learning in Finance: From Theory to Practice",
  author    = "Dixon, Matthew F and Halperin, Igor and Bilokon, Paul",
  abstract  = "This book introduces machine learning methods in finance. It
               presents a unified treatment of machine learning and various
               statistical and computational disciplines in quantitative
               finance, such as financial econometrics and discrete time
               stochastic control, with an emphasis on how theory and
               hypothesis tests inform the choice of algorithm for financial
               data modeling and decision making. With the trend towards
               increasing computational resources and larger datasets, machine
               learning has grown into an important skillset for the finance
               industry. This book is written for advanced graduate students
               and academics in financial econometrics, mathematical finance
               and applied statistics, in addition to quants and data
               scientists in the field of quantitative finance. Machine
               Learning in Finance: From Theory to Practice is divided into
               three parts, each part covering theory and applications. The
               first presents supervised learning for cross-sectional data from
               both a Bayesian and frequentist perspective. The more advanced
               material places a firm emphasis on neural networks, including
               deep learning, as well as Gaussian processes, with examples in
               investment management and derivative modeling. The second part
               presents supervised learning for time series data, arguably the
               most common data type used in finance with examples in trading,
               stochastic volatility and fixed income modeling. Finally, the
               third part presents reinforcement learning and its applications
               in trading, investment and wealth management. Python code
               examples are provided to support the readers' understanding of
               the methodologies and applications. The book also includes more
               than 80 mathematical and programming exercises, with worked
               solutions available to instructors. As a bridge to research in
               this emergent field, the final chapter presents the frontiers of
               machine learning in finance from a researcher's perspective,
               highlighting how many well-known concepts in statistical physics
               are likely to emerge as important methodologies for machine
               learning in finance.",
  publisher = "Springer International Publishing",
  month     =  jul,
  year      =  2020,
  language  = "en"
}


@ARTICLE{Blume2019,
  title     = "An introduction to second-generation p-values",
  author    = "Blume, Jeffrey D and Greevy, Robert A and Welty, Valerie F and
               Smith, Jeffrey R and Dupont, William D",
  journal   = "Am. Stat.",
  publisher = "Informa UK Limited",
  volume    =  73,
  number    = "sup1",
  pages     = "157--167",
  month     =  mar,
  year      =  2019,
  copyright = "http://creativecommons.org/licenses/by-nc-nd/4.0/",
  language  = "en"
}

@ARTICLE{Fisher1936,
  title     = "Design of Experiments",
  author    = "Fisher, R A",
  journal   = "Br. Med. J.",
  publisher = "British Medical Journal Publishing Group",
  volume    =  1,
  number    =  3923,
  pages     = "554--554",
  month     =  mar,
  year      =  1936,
  language  = "en"
}


@ARTICLE{Knuth1984,
  title     = "Literate Programming",
  author    = "Knuth, D E",
  abstract  = "Abstract. The author and his associates have been experimenting
               for the past several years with a programming language and
               documentation system called WEB. This",
  journal   = "Comput. J.",
  publisher = "Oxford Academic",
  volume    =  27,
  number    =  2,
  pages     = "97--111",
  month     =  jan,
  year      =  1984,
  language  = "en"
}


@ARTICLE{Heskes2020,
  title         = "Causal Shapley Values: Exploiting Causal Knowledge to
                   Explain Individual Predictions of Complex Models",
  author        = "Heskes, Tom and Sijben, Evi and Bucur, Ioan Gabriel and
                   Claassen, Tom",
  abstract      = "Shapley values underlie one of the most popular
                   model-agnostic methods within explainable artificial
                   intelligence. These values are designed to attribute the
                   difference between a model's prediction and an average
                   baseline to the different features used as input to the
                   model. Being based on solid game-theoretic principles,
                   Shapley values uniquely satisfy several desirable
                   properties, which is why they are increasingly used to
                   explain the predictions of possibly complex and highly
                   non-linear machine learning models. Shapley values are well
                   calibrated to a user's intuition when features are
                   independent, but may lead to undesirable, counterintuitive
                   explanations when the independence assumption is violated.
                   In this paper, we propose a novel framework for computing
                   Shapley values that generalizes recent work that aims to
                   circumvent the independence assumption. By employing Pearl's
                   do-calculus, we show how these 'causal' Shapley values can
                   be derived for general causal graphs without sacrificing any
                   of their desirable properties. Moreover, causal Shapley
                   values enable us to separate the contribution of direct and
                   indirect effects. We provide a practical implementation for
                   computing causal Shapley values based on causal chain graphs
                   when only partial information is available and illustrate
                   their utility on a real-world example.",
  month         =  nov,
  year          =  2020,
  archivePrefix = "arXiv",
  primaryClass  = "cs.AI",
  eprint        = "2011.01625"
}


@INPROCEEDINGS{Sundararajan2020,
  title     = "The Many Shapley Values for Model Explanation",
  booktitle = "Proceedings of the 37th International Conference on Machine
               Learning",
  author    = "Sundararajan, Mukund and Najmi, Amir",
  editor    = "Iii, Hal Daum{\'e} and Singh, Aarti",
  abstract  = "The Shapley value has become the basis for several methods that
               attribute the prediction of a machine-learning model on an input
               to its base features. The use of the Shapley value is justified
               by citing the uniqueness result from
               \textbackslashciteShapley53, which shows that it is the only
               method that satisfies certain good properties
               (\textbackslashemphaxioms). There are, however, a multiplicity
               of ways in which the Shapley value is operationalized for model
               explanation. These differ in how they reference the model, the
               training data, and the explanation context. Hence they differ in
               output, rendering the uniqueness result inapplicable.
               Furthermore, the techniques that rely on they training data
               produce non-intuitive attributions, for instance unused features
               can still receive attribution. In this paper, we use the
               axiomatic approach to study the differences between some of the
               many operationalizations of the Shapley value for attribution.
               We discuss a technique called Baseline Shapley (BShap), provide
               a proper uniqueness result for it, and contrast it with two
               other techniques from prior literature, Integrated Gradients
               \textbackslashciteSTY17 and Conditional Expectation Shapley
               \textbackslashciteLundberg2017AUA.",
  publisher = "PMLR",
  volume    =  119,
  pages     = "9269--9278",
  series    = "Proceedings of Machine Learning Research",
  year      =  2020
}


@ARTICLE{Zuo2021,
  title     = "Variable selection with second-generation P-values",
  author    = "Zuo, Yi and Stewart, Thomas G and Blume, Jeffrey D",
  journal   = "Am. Stat.",
  publisher = "Informa UK Limited",
  pages     = "1--21",
  month     =  jun,
  year      =  2021,
  language  = "en"
}


@ARTICLE{De_Prado2019,
  title     = "A Data Science Solution to the {Multiple-Testing} Crisis in
               Financial Research",
  author    = "L{\'o}pez de Prado, Marcos",
  journal   = "The Journal of Financial Data Science",
  publisher = "Institutional Investor Journals Umbrella",
  volume    =  1,
  number    =  1,
  pages     = "99--110",
  year      =  2019
}

@ARTICLE{Mahdavi2020,
  title     = "It's all about data: How to make good decisions in a world awash
               with information",
  author    = "Mahdavi, Mehrzad and Kazemi, Hossein",
  journal   = "The Journal of Financial Data Science",
  publisher = "Pageant Media US",
  volume    =  2,
  number    =  2,
  pages     = "8--16",
  month     =  apr,
  year      =  2020,
  language  = "en"
}

@ARTICLE{Merton1973,
  title     = "Theory of Rational Option Pricing",
  author    = "Merton, Robert C",
  abstract  = "[The long history of the theory of option pricing began in 1900
               when the French mathematician Louis Bachelier deduced an option
               pricing formula based on the assumption that stock prices follow
               a Brownian motion with zero drift. Since that time, numerous
               researchers have contributed to the theory. The present paper
               begins by deducing a set of restrictions on option pricing
               formulas from the assumption that investors prefer more to less.
               These restrictions are necessary conditions for a formula to be
               consistent with a rational pricing theory. Attention is given to
               the problems created when dividends are paid on the underlying
               common stock and when the terms of the option contract can be
               changed explicitly by a change in exercise price or implicitly
               by a shift in the investment or capital structure policy of the
               firm. Since the deduced restrictions are not sufficient to
               uniquely determine an option pricing formula, additional
               assumptions are introduced to examine and extend the seminal
               Black-Scholes theory of option pricing. Explicit formulas for
               pricing both call and put options as well as for warrants and
               the new ``down-and-out'' option are derived. The effects of
               dividends and call provisions on the warrant price are examined.
               The possibilities for further extension of the theory to the
               pricing of corporate liabilities are discussed.]",
  journal   = "The Bell Journal of Economics and Management Science",
  publisher = "[Wiley, RAND Corporation]",
  volume    =  4,
  number    =  1,
  pages     = "141--183",
  year      =  1973
}

@ARTICLE{Molina2019,
  title     = "Machine Learning for Sociology",
  author    = "Molina, Mario and Garip, Filiz",
  abstract  = "Machine learning is a field at the intersection of statistics
               and computer science that uses algorithms to extract information
               and knowledge from data. Its applications increasingly find
               their way into economics, political science, and sociology. We
               offer a brief introduction to this vast toolbox and illustrate
               its current uses in the social sciences, including distilling
               measures from new data sources, such as text and images;
               characterizing population heterogeneity; improving causal
               inference; and offering predictions to aid policy decisions and
               theory development. We argue that, in addition to serving
               similar purposes in sociology, machine learning tools can speak
               to long-standing questions on the limitations of the linear
               modeling framework, the criteria for evaluating empirical
               findings, transparency around the context of discovery, and the
               epistemological core of the discipline.",
  journal   = "Annu. Rev. Sociol.",
  publisher = "Annual Reviews",
  volume    =  45,
  number    =  1,
  pages     = "27--45",
  month     =  jul,
  year      =  2019
}

@INCOLLECTION{Reisinger2018,
  title     = "Finite difference methods for medium-and high-dimensional
               derivative pricing {PDEs}",
  booktitle = "{High-Performance} Computing in Finance",
  author    = "Reisinger, Christoph and Wissmann, Rasmus",
  publisher = "Chapman and Hall/CRC",
  pages     = "175--195",
  year      =  2018
}

@BOOK{Spiegelhalter2019,
  title     = "The Art of Statistics: Learning from Data",
  author    = "Spiegelhalter, David",
  abstract  = "'A statistical national treasure' Jeremy Vine, BBC Radio
               2'Required reading for all politicians, journalists, medics and
               anyone who tries to influence people (or is influenced) by
               statistics. A tour de force' Popular ScienceDo busier hospitals
               have higher survival rates? How many trees are there on the
               planet? Why do old men have big ears? David Spiegelhalter
               reveals the answers to these and many other questions -
               questions that can only be addressed using statistical
               science.Statistics has played a leading role in our scientific
               understanding of the world for centuries, yet we are all
               familiar with the way statistical claims can be sensationalised,
               particularly in the media. In the age of big data, as data
               science becomes established as a discipline, a basic grasp of
               statistical literacy is more important than ever. In The Art of
               Statistics, David Spiegelhalter guides the reader through the
               essential principles we need in order to derive knowledge from
               data. Drawing on real world problems to introduce conceptual
               issues, he shows us how statistics can help us determine the
               luckiest passenger on the Titanic, whether serial killer Harold
               Shipman could have been caught earlier, and if screening for
               ovarian cancer is beneficial. 'Shines a light on how we can use
               the ever-growing deluge of data to improve our understanding of
               the world' Nature",
  publisher = "Penguin UK",
  month     =  mar,
  year      =  2019,
  language  = "en"
}

@ARTICLE{Wager2017,
  title     = "Estimation and Inference of Heterogeneous Treatment Effects
               using Random Forests",
  author    = "Wager, Stefan and Athey, Susan",
  abstract  = "ABSTRACTMany scientific and engineering challenges?ranging from
               personalized medicine to customized marketing
               recommendations?require an understanding of treatment effect
               heterogeneity. In this article, we develop a nonparametric
               causal forest for estimating heterogeneous treatment effects
               that extends Breiman?s widely used random forest algorithm. In
               the potential outcomes framework with unconfoundedness, we show
               that causal forests are pointwise consistent for the true
               treatment effect and have an asymptotically Gaussian and
               centered sampling distribution. We also discuss a practical
               method for constructing asymptotic confidence intervals for the
               true treatment effect that are centered at the causal forest
               estimates. Our theoretical results rely on a generic Gaussian
               theory for a large family of random forest algorithms. To our
               knowledge, this is the first set of results that allows any type
               of random forest, including classification and regression
               forests, to be used for provably valid statistical inference. In
               experiments, we find causal forests to be substantially more
               powerful than classical methods based on nearest-neighbor
               matching, especially in the presence of irrelevant covariates.",
  journal   = "J. Am. Stat. Assoc.",
  publisher = "Taylor \& Francis",
  pages     = "1--15",
  month     =  apr,
  year      =  2017
}

@ARTICLE{Zuo2021,
  title     = "Variable selection with second-generation P-values",
  author    = "Zuo, Yi and Stewart, Thomas G and Blume, Jeffrey D",
  journal   = "Am. Stat.",
  publisher = "Informa UK Limited",
  pages     = "1--21",
  month     =  jun,
  year      =  2021,
  language  = "en"
}

@MISC{RStudioW2021,
  title        = "{RStudio} Workbench",
  abstract     = "Build great data science products",
  year         =  2021,
  howpublished = "\url{https://www.rstudio.com/products/workbench/}",
  note         = "Accessed: 2021-7-19"
}

@MISC{RStudioT2021,
  title        = "{RStudio} Team",
  abstract     = "A single home for R and Python Data Science Teams.",
  year         =  2021,
  howpublished = "\url{https://www.rstudio.com/products/team/}",
  note         = "Accessed: 2021-8-9"
}

@ARTICLE{Lin2017,
  title     = "Incorporated risk metrics and hybrid {AI} techniques for risk
               management",
  author    = "Lin, Sin-Jin and Hsu, Ming-Fu",
  abstract  = "This study proposes a novel technique by extending balanced
               scorecards with risk management considerations (i.e., risk
               metrics and insolvency risk) for corporate operating performance
               assessment and then establishes a fusion mechanism that
               incorporates hybrid filter-wrapper subset selection (HFW),
               random vector functional-link network (RVFLN), and ant colony
               optimization (ACO) for operating performance forecasting. The
               study executes HFW, which preserves the advantages of wrapper
               approaches, but prevents paying its tremendous computational
               cost, in order to determine the essential features for
               forecasting model construction. With the merits of rapid
               learning speed and no extra inherent parameters needed to be
               tuned, RVFLN helps establish the forecasting model. However,
               RVFLN has demonstrated that its superior forecasting performance
               comes with the challenge of being unable to represent the
               inherent decision logic for humans to comprehend. To cope with
               this task, the study conducts ACO so as to extract the inherent
               knowledge from RVFLN and represents it in human-readable format.
               If the extracted knowledge is not comprehensive for decision
               makers, then they will not be able to interpret and verify it.
               In this circumstance, the decision makers probably will not
               trust enough the extracted knowledge and be prone to making
               unreliable judgments more easily. The introduced mechanism
               herein is examined by real cases and poses superior forecasting
               quality under numerous examinations. It is a promising
               alternative for corporate operating performance forecasting.",
  journal   = "Neural Comput. Appl.",
  publisher = "Springer",
  volume    =  28,
  number    =  11,
  pages     = "3477--3489",
  month     =  nov,
  year      =  2017
}
@article{Hang.2019, 
year = {2019}, 
keywords = {CAPM,Rare disasters,Measurement errors,Consumption CAPM,General equilibrium,FactorAnalysis}, 
title = {{The CAPM strikes back? An equilibrium model with disasters}}, 
author = {Hang, Bai, and Kewei, Hou, and Howard, Kung, and N, Li, Erica X and Lu, Zhang,}, 
journal = {J. financ. econ.}, 
issn = {0304-405X}, 
doi = {10.1016/j.jfineco.2018.08.009}, 
url = {https://www.sciencedirect.com/science/article/pii/S0304405X18302307}, 
abstract = {{Embedding disasters into a general equilibrium model with heterogeneous      firms induces strong nonlinearity in the pricing kernel, helping explain      the empirical failure of the (consumption) CAPM. Our single-factor model      reproduces the failure of the CAPM in explaining the value premium in      finite samples without disasters and its relative success in samples with      disasters. Due to beta measurement errors, the estimated beta-return      relation is flat, consistent with the beta â€œanomaly,â€ even though the true      beta-return relation is strongly positive. Finally, the consumption CAPM      fails in simulations, even though a nonlinear model with the true pricing      kernel holds exactly by construction.}}, 
pages = {269--298}, 
number = {2}, 
volume = {131}, 
month = {2}
}
@article{Campbell.2017, 
year = {2017}, 
keywords = {P-hacking, Multiple testing, Selection, Data mining, Data dredging, Rare incidence, Type I error, Type II error, P-values, Minimum Bayes Factor, MBF, SD-MBF, Bayesian P-values,FML_reading}, 
title = {{Presidential Address: The Scientific Outlook in Financial Economics}}, 
author = {R, Harvey, Campbell}, 
journal = {SSRN Electronic Journal}, 
doi = {10.2139/ssrn.2893930}, 
url = {https://papers.ssrn.com/sol3/papers.cfm?abstract\_id=2893930}, 
abstract = {{Given the competition for top journal space, there is an incentive to      produce â€œsignificantâ€ results. With the combination of unreported tests,      lack of adjustment for multiple tests, and direct and indirect p-hacking,      many of the results being published will fail to hold up in the future. In      addition, there are basic issues with the interpretation of statistical      significance. Increasing thresholds may be necessary, but still may not be      sufficient: if the effect being studied is rare, even t > 3 will produce a      large number of false positives. Here I explore the meaning and      limitations of a p-value. I offer a simple alternative (the minimum Bayes      factor). I present guidelines for a robust, transparent research culture      in financial economics. Finally, I offer some thoughts on the importance      of risk taking (from the perspective of authors and editors) to advance      our field. The transcript and presentation slides are available here:      http://ssrn.com/abstract=2895842.}}, 
month = {7}
}
@article{L.2016uks, 
year = {2016}, 
keywords = {0*,Teaching Econometrics,TeachingStatistics,NHST}, 
title = {{The ASA's Statement on p-Values: Context, Process, and Purpose}}, 
author = {L, Wasserstein, Ronald and A, Lazar, Nicole}, 
journal = {Am. Stat.}, 
issn = {0003-1305}, 
doi = {10.1080/00031305.2016.1154108}, 
url = {http://www.tandfonline.com/doi/pdf/10.1080/00031305.2016.1154108?t=}, 
pages = {129--133}, 
number = {2}, 
volume = {70}, 
note = {doi: 10.1080/00031305.2016.1154108}, 
month = {4}
}

@BOOK{Gelman2020,
  title     = "Regression and Other Stories",
  author    = "Gelman, Andrew and Hill, Jennifer and Vehtari, Aki",
  abstract  = "Most textbooks on regression focus on theory and the simplest of
               examples. Real statistical problems, however, are complex and
               subtle. This is not a book about the theory of regression. It is
               about using regression to solve real problems of comparison,
               estimation, prediction, and causal inference. Unlike other
               books, it focuses on practical issues such as sample size and
               missing data and a wide range of goals and techniques. It jumps
               right in to methods and computer code you can use immediately.
               Real examples, real stories from the authors' experience
               demonstrate what regression can do and its limitations, with
               practical advice for understanding assumptions and implementing
               methods for experiments and observational studies. They make a
               smooth transition to logistic regression and GLM. The emphasis
               is on computation in R and Stan rather than derivations, with
               code available online. Graphics and presentation aid
               understanding of the models and model fitting.",
  publisher = "Cambridge University Press",
  month     =  jul,
  year      =  2020,
  language  = "en"
}


@ARTICLE{Cetinkaya-Rundel2018,
  title     = "Infrastructure and tools for teaching computing throughout the
               statistical curriculum",
  author    = "{\c C}etinkaya-Rundel, Mine and Rundel, Colin",
  journal   = "Am. Stat.",
  publisher = "Informa UK Limited",
  volume    =  72,
  number    =  1,
  pages     = "58--65",
  month     =  jan,
  year      =  2018,
  language  = "en"
}
